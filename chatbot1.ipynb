{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chatbot1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMQMAkvDbsQFM+9rdUhGA8z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Masoud-Ghasemian/PyTorch_Basics/blob/master/chatbot1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtsHUdKrQFRS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "af7d475a-691a-4500-b758-daa48b73f76e"
      },
      "source": [
        "import torch\n",
        "import random\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import nltk\n",
        "import numpy as np\n",
        "nltk.download('punkt')\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "stemmer = PorterStemmer()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iU2YRbcQpW9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(sentence):\n",
        "  return nltk.word_tokenize(sentence)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrpaIf0URUgo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5770bcc5-5851-4687-96b0-c2dc6139b17e"
      },
      "source": [
        "sentence = 'hey how are you'\n",
        "tokenize(sentence)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hey', 'how', 'are', 'you']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivXlldwERbUo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stem(word):\n",
        "  return stemmer.stem(word.lower())"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2Lbqy8iSkxv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0c1efe12-f7ee-4f32-d78c-0e42fbe3c884"
      },
      "source": [
        "words = ['dogs', 'Cat', 'talked']\n",
        "stemmed_words = [stem(word) for word in words]\n",
        "print(stemmed_words)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['dog', 'cat', 'talk']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-1Urv6auQXE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bag_of_words(tokenized_sentence, all_words):\n",
        "  bag = np.zeros(len(all_words), dtype=np.float32)\n",
        "  tokenized_sentence = [stem(w) for w in tokenized_sentence]\n",
        "  for idx, w in enumerate(all_words):\n",
        "    if w in tokenized_sentence:\n",
        "      bag[idx] = 1.0\n",
        "\n",
        "  return bag"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Sp2CwBWSpu4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "923cd416-e379-4b64-d21a-2d7931c8f2f6"
      },
      "source": [
        "import json\n",
        "with open('intents.json', 'r') as f:\n",
        "  intents = json.load(f)\n",
        "\n",
        "all_words = []\n",
        "tags = []\n",
        "xy = []\n",
        "for intent in intents['intents']:\n",
        "  tag = intent['tag']\n",
        "  tags.append(tag)\n",
        "  for pattern in intent['patterns']:\n",
        "    w = tokenize(pattern)\n",
        "    all_words.extend(w)\n",
        "    xy.append((w, tag))\n",
        "ignore_words = ['?', '!', ',', '.']\n",
        "all_words = [stem(w) for w in all_words if w not in ignore_words]\n",
        "all_words = sorted(set(all_words))\n",
        "tags = sorted(set(tags))\n",
        "print(all_words)\n",
        "print(tags)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-38fc95eed27f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'intents.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mintents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mall_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'intents.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dno8gdn5VPon",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = []\n",
        "y_train = []\n",
        "\n",
        "for (pattern_sentence, tag) in xy:\n",
        "  bag = bag_of_words(pattern_sentence, all_words)\n",
        "  X_train.append(bag)\n",
        "\n",
        "  label = tags.index(tag)\n",
        "  y_train.append(label)\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fItBg2nxgmZ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(all_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1O0s36NN2uQe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ChatDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    self.n_samples = len(X_train)\n",
        "    self.x_data = X_train\n",
        "    self.y_data = y_train\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.x_data[index], self.y_data[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.n_samples\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_p0HRSL15SE4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 8\n",
        "hidden_size = 8\n",
        "output_size = len(tags)\n",
        "input_size = len(X_train[0])\n",
        "learning_rate = 0.001\n",
        "\n",
        "\n",
        "\n",
        "dataset = ChatDataset()\n",
        "train_loader = DataLoader(dataset = dataset, batch_size=batch_size, shuffle=True, num_workers = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gUCMyMO5s81",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NeuralNet(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_classes):\n",
        "    super(NeuralNet, self).__init__()\n",
        "    self.l1 = nn.Linear(input_size, hidden_size)\n",
        "    self.l2 = nn.Linear(hidden_size, hidden_size)\n",
        "    self.l3 = nn.Linear(hidden_size, num_classes)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.relu(self.l1(x))\n",
        "    out = self.relu(self.l2(out))\n",
        "    return self.l3(out)\n",
        "    "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElulloRY8eKm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.adam(model.parapeters(), lr = learning_rate)\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  for (words, labels) in train_loader:\n",
        "    words = words.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    #forwards\n",
        "    outputs = model(words)\n",
        "    loss = criterion(labels, outputs)\n",
        "\n",
        "    #backward\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  if (epoch + 1) % 100 == 0:\n",
        "    print(f'epoch {epoch + 1}/{num_epochs}, loss={loss.item():.4f}')\n",
        "\n",
        "  \n",
        "data = {\n",
        "    \"model_sate\": model.state_dict(),\n",
        "    \"input_size\": input_size,\n",
        "    \"output_size\": output_size,\n",
        "    \"hidden_size\": hidden_size,\n",
        "    \"all_words\": all_words,\n",
        "    \"tags\" : tags\n",
        "}\n",
        "\n",
        "\n",
        "FILE = 'data.pth'\n",
        "torch.save(data, FILE)\n",
        "\n",
        "print(f'training complete. file saved to {FILE}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uec9NbY3RVxv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}